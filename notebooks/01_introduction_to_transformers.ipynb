{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction aux Transformers\n",
    "=============================\n",
    "\n",
    "Bienvenue dans le premier notebook de la série “Transformers Explained”\n",
    "! Dans ce notebook, nous allons explorer les bases de l’architecture des\n",
    "Transformers, un modèle qui a révolutionné le traitement du langage\n",
    "naturel (TLN) et d’autres domaines.\n",
    "\n",
    "Qu’est-ce qu’un Transformer ?\n",
    "-----------------------------\n",
    "\n",
    "Le Transformer est une architecture de réseau neuronal introduite en\n",
    "2017 par Vaswani et al. dans l’article “Attention Is All You Need”.\n",
    "Contrairement aux modèles précédents comme les RNN (réseaux de neurones\n",
    "récurrents) et les CNN (réseaux de neurones convolutifs), le Transformer\n",
    "s’appuie entièrement sur des mécanismes d’attention, éliminant ainsi la\n",
    "nécessité de récurrence ou de convolutions.\n",
    "\n",
    "Pourquoi les Transformers sont-ils importants ?\n",
    "-----------------------------------------------\n",
    "\n",
    "Avant les Transformers, les modèles séquentiels comme les RNN (LSTM,\n",
    "GRU) étaient dominants pour les tâches de TLN. Cependant, ils\n",
    "souffraient de limitations, notamment la difficulté à traiter les\n",
    "dépendances à longue portée et la parallélisation limitée. Les\n",
    "Transformers ont résolu ces problèmes en introduisant le mécanisme\n",
    "d’auto-attention, permettant au modèle de traiter toutes les parties\n",
    "d’une séquence simultanément.\n",
    "\n",
    "Architecture Générale\n",
    "---------------------\n",
    "\n",
    "Un Transformer typique se compose de deux parties principales :\n",
    "\n",
    "1.  **Encodeur** : Traite la séquence d’entrée et produit une\n",
    "    représentation contextuelle.\n",
    "2.  **Décodeur** : Utilise la représentation de l’encodeur pour générer\n",
    "    la séquence de sortie.\n",
    "\n",
    "Chaque encodeur et décodeur est composé de plusieurs couches identiques\n",
    "empilées.\n",
    "\n",
    "### Blocs d’Encodeur\n",
    "\n",
    "Chaque bloc d’encodeur contient deux sous-couches :\n",
    "\n",
    "-   **Mécanisme d’auto-attention multi-têtes** : Permet au modèle de\n",
    "    pondérer différentes parties de la séquence d’entrée.\n",
    "-   **Réseau de neurones feed-forward positionnel** : Applique une\n",
    "    transformation linéaire à chaque position.\n",
    "\n",
    "### Blocs de Décodeur\n",
    "\n",
    "Chaque bloc de décodeur contient trois sous-couches :\n",
    "\n",
    "-   **Mécanisme d’auto-attention masqué multi-têtes** : Similaire à\n",
    "    l’encodeur, mais masque les positions futures pour éviter la triche.\n",
    "-   **Mécanisme d’attention multi-têtes Encodeur-Décodeur** : Permet au\n",
    "    décodeur de se concentrer sur des parties pertinentes de la sortie\n",
    "    de l’encodeur.\n",
    "-   **Réseau de neurones feed-forward positionnel**.\n",
    "\n",
    "Encodage Positionnel\n",
    "--------------------\n",
    "\n",
    "Puisque les Transformers ne contiennent pas de récurrence ou de\n",
    "convolution, ils n’ont intrinsèquement aucune notion de l’ordre des mots\n",
    "dans la séquence. Pour remédier à cela, des “encodages positionnels”\n",
    "sont ajoutés aux embeddings d’entrée. Ces encodages fournissent des\n",
    "informations sur la position relative ou absolue des tokens dans la\n",
    "séquence.\n",
    "\n",
    "Exemple Simplifié (Pseudo-code)\n",
    "-------------------------------\n",
    "\n",
    "``` python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "# Un exemple très simplifié d'une couche d'encodeur\n",
    "class SimpleEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, dropout):\n",
    "        super(SimpleEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(torch.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "# Dimensions du modèle\n",
    "d_model = 512  # Dimension des embeddings\n",
    "num_heads = 8  # Nombre de têtes d'attention\n",
    "dim_feedforward = 2048 # Dimension du réseau feed-forward\n",
    "dropout = 0.1\n",
    "\n",
    "# Création d'une couche d'encodeur simple\n",
    "encoder_layer = SimpleEncoderLayer(d_model, num_heads, dim_feedforward, dropout)\n",
    "\n",
    "# Création d'un encodage positionnel\n",
    "pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "# Exemple d'entrée (batch_size, sequence_length, d_model)\n",
    "# Supposons une séquence de 10 mots, avec un batch de 2\n",
    "input_sequence = torch.rand(10, 2, d_model)\n",
    "\n",
    "# Ajout de l'encodage positionnel\n",
    "input_with_pos = pos_encoder(input_sequence)\n",
    "\n",
    "# Passage à travers la couche d'encodeur\n",
    "output = encoder_layer(input_with_pos)\n",
    "\n",
    "print(f\"Shape de l'entrée: {input_sequence.shape}\")\n",
    "print(f\"Shape de la sortie de l'encodeur: {output.shape}\")\n",
    "```\n",
    "\n",
    "Conclusion\n",
    "----------\n",
    "\n",
    "Ce notebook a fourni une introduction aux concepts fondamentaux des\n",
    "Transformers. Dans les notebooks suivants, nous plongerons plus en\n",
    "détail dans le mécanisme d’attention et construirons un Transformer\n",
    "complet étape par étape."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
