{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le mécanisme d’attention\n",
    "========================\n",
    "\n",
    "Bienvenue dans le deuxième notebook de la série “Transformers Explained”\n",
    "! Dans ce notebook, nous allons plonger au cœur de l’architecture des\n",
    "Transformers : le mécanisme d’attention, et plus particulièrement\n",
    "l’auto-attention (self-attention).\n",
    "\n",
    "Qu’est-ce que l’attention ?\n",
    "---------------------------\n",
    "\n",
    "Dans le contexte des réseaux de neurones, l’attention est un mécanisme\n",
    "qui permet au modèle de pondérer l’importance de différentes parties de\n",
    "la séquence d’entrée (ou de sortie) lors de la prédiction. Cela permet\n",
    "au modèle de se concentrer sur les informations les plus pertinentes,\n",
    "plutôt que de traiter toutes les informations de manière égale.\n",
    "\n",
    "L’auto-attention (Self-Attention)\n",
    "---------------------------------\n",
    "\n",
    "L’auto-attention est un type d’attention où le modèle apprend à pondérer\n",
    "les relations entre les différentes positions d’une *même* séquence.\n",
    "Cela signifie que chaque mot dans une phrase peut “regarder” les autres\n",
    "mots de la phrase pour mieux comprendre son propre contexte.\n",
    "\n",
    "### Comment ça marche ?\n",
    "\n",
    "Le mécanisme d’auto-attention calcule trois vecteurs pour chaque mot\n",
    "d’entrée :\n",
    "\n",
    "1.  **Query (Requête - Q)** : Représente ce que le mot actuel recherche.\n",
    "2.  **Key (Clé - K)** : Représente ce que le mot actuel offre.\n",
    "3.  **Value (Valeur - V)** : Contient l’information réelle du mot.\n",
    "\n",
    "Le calcul de l’auto-attention se déroule en plusieurs étapes :\n",
    "\n",
    "1.  **Calcul des scores de similarité** : Pour chaque mot, on calcule un\n",
    "    score de similarité entre sa Query et les Keys de tous les autres\n",
    "    mots (y compris lui-même). Cela se fait généralement par un produit\n",
    "    scalaire.\n",
    "2.  **Mise à l’échelle** : Les scores sont divisés par la racine carrée\n",
    "    de la dimension des Keys (pour stabiliser les gradients).\n",
    "3.  **Softmax** : Une fonction softmax est appliquée aux scores mis à\n",
    "    l’échelle pour obtenir des poids d’attention. Ces poids indiquent\n",
    "    l’importance de chaque mot pour le mot actuel.\n",
    "4.  **Pondération des valeurs** : Les poids d’attention sont multipliés\n",
    "    par les vecteurs Value correspondants. La somme de ces produits\n",
    "    pondérés donne le vecteur de sortie pour le mot actuel.\n",
    "\n",
    "### Formule de l’auto-attention\n",
    "\n",
    "La formule de l’auto-attention est la suivante :\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
    "\n",
    "Où : - $Q$ est la matrice des requêtes. - $K$ est la matrice des clés. -\n",
    "$V$ est la matrice des valeurs. - $d_k$ est la dimension des clés.\n",
    "\n",
    "Auto-attention multi-têtes (Multi-Head Attention)\n",
    "-------------------------------------------------\n",
    "\n",
    "L’auto-attention multi-têtes est une extension du mécanisme\n",
    "d’auto-attention. Au lieu d’effectuer un seul calcul d’attention, le\n",
    "modèle effectue plusieurs calculs d’attention en parallèle (chaque\n",
    "“tête” d’attention). Les résultats de ces têtes sont ensuite concaténés\n",
    "et transformés linéairement.\n",
    "\n",
    "Cela permet au modèle de se concentrer sur différentes relations et\n",
    "aspects de la séquence simultanément, enrichissant ainsi sa capacité à\n",
    "capturer des dépendances complexes.\n",
    "\n",
    "Exemple Simplifié (Pseudo-code)\n",
    "-------------------------------\n",
    "\n",
    "``` python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0] # Batch size\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Einsum does matrix multiplication for query * key.T\n",
    "        # sum over the last two dimensions\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nkhq\", [queries, keys]) # (N, heads, query_len, key_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nkhq,nvhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "# Dimensions du modèle\n",
    "embed_size = 256\n",
    "heads = 8\n",
    "sequence_length = 10\n",
    "batch_size = 2\n",
    "\n",
    "# Création d'une instance de SelfAttention\n",
    "attention_layer = SelfAttention(embed_size, heads)\n",
    "\n",
    "# Exemple d'entrée (batch_size, sequence_length, embed_size)\n",
    "# Supposons une séquence de 10 mots, avec un batch de 2\n",
    "input_data = torch.rand(batch_size, sequence_length, embed_size)\n",
    "\n",
    "# Passage à travers la couche d'attention\n",
    "# Pour l'auto-attention, values, keys et query sont les mêmes\n",
    "output = attention_layer(input_data, input_data, input_data, mask=None)\n",
    "\n",
    "print(f\"Shape de l'entrée: {input_data.shape}\")\n",
    "print(f\"Shape de la sortie de l'attention: {output.shape}\")\n",
    "```\n",
    "\n",
    "Conclusion\n",
    "----------\n",
    "\n",
    "Le mécanisme d’attention est la pierre angulaire des Transformers, leur\n",
    "permettant de traiter les informations de manière non séquentielle et de\n",
    "capturer des dépendances complexes. L’auto-attention multi-têtes\n",
    "améliore encore cette capacité en permettant au modèle de se concentrer\n",
    "sur diverses relations simultanément.\n",
    "\n",
    "Dans le prochain notebook, nous utiliserons ces concepts pour construire\n",
    "un Transformer complet à partir de zéro."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
