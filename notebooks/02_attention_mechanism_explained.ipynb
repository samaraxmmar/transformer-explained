# The Attention Mechanism
=========================

Welcome to the second notebook of the “Transformers Explained” series!  
In this notebook, we will dive deep into the core of Transformer architecture: the attention mechanism, and more specifically, **self-attention**.

---

## What is Attention?
---------------------

In neural networks, **attention** is a mechanism that allows the model to weigh the importance of different parts of the input (or output) sequence during prediction.  
This enables the model to **focus on the most relevant information** instead of treating all words equally.



---

## Self-Attention
-----------------

**Self-attention** is a type of attention where the model learns to weigh the relationships between different positions within the **same input sequence**.  
In other words, **each word in a sentence can look at the other words** to better understand its own meaning.



---

### How Does Self-Attention Work?

The self-attention mechanism computes **three vectors** for each input word:

1. **Query (Q)** – What the word is looking for  
2. **Key (K)** – What the word offers  
3. **Value (V)** – The actual content of the word

The steps are:

1. **Compute similarity scores** between the query and all keys using dot product  
2. **Scale** the scores by \(\sqrt{d_k}\)  
3. **Apply softmax** to get attention weights  
4. **Weight values** by multiplying each Value by the corresponding attention weight  
5. **Sum weighted values** to get the final output vector for each word

---

### Self-Attention Formula

\[
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V
\]

Where:
- \( Q \): query matrix  
- \( K \): key matrix  
- \( V \): value matrix  
- \( d_k \): dimensionality of the keys

---

## Multi-Head Attention
------------------------

**Multi-head attention** is an extension of self-attention.  
Instead of performing one attention calculation, the model performs **multiple attention operations in parallel**, known as "heads".  
Their outputs are concatenated and passed through a linear transformation.

This allows the model to focus on **different types of relationships** simultaneously.


---

## Simplified Example (Code)
----------------------------

```python
import torch
import torch.nn as nn
import math

class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads

        assert self.head_dim * heads == embed_size, "Embedding size must be divisible by number of heads"

        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)

    def forward(self, values, keys, query, mask):
        N = query.shape[0]  # Batch size
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]

        # Split embeddings into multiple heads
        values = values.reshape(N, value_len, self.heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.heads, self.head_dim)
        queries = query.reshape(N, query_len, self.heads, self.head_dim)

        values = self.values(values)
        keys = self.keys(keys)
        queries = self.queries(queries)

        # Dot product attention
        energy = torch.einsum("nqhd,nkhd->nkhq", [queries, keys])

        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20"))

        attention = torch.softmax(energy / math.sqrt(self.head_dim), dim=3)

        out = torch.einsum("nkhq,nvhd->nqhd", [attention, values])
        out = out.reshape(N, query_len, self.heads * self.head_dim)

        return self.fc_out(out)

# Model configuration
embed_size = 256
heads = 8
sequence_length = 10
batch_size = 2

# Self-Attention layer
attention_layer = SelfAttention(embed_size, heads)

# Example input
input_data = torch.rand(batch_size, sequence_length, embed_size)

# Apply self-attention
output = attention_layer(input_data, input_data, input_data, mask=None)

print(f"Input shape: {input_data.shape}")
print(f"Output shape: {output.shape}")
