{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construire un Transformer à partir de zéro\n",
    "==========================================\n",
    "\n",
    "Bienvenue dans le troisième notebook de la série “Transformers\n",
    "Explained” ! Après avoir exploré les bases des Transformers et le\n",
    "mécanisme d’attention, nous allons maintenant assembler ces pièces pour\n",
    "construire un Transformer complet à partir de zéro.\n",
    "\n",
    "Ce notebook se concentrera sur la mise en œuvre des composants clés d’un\n",
    "Transformer, y compris l’encodeur, le décodeur, l’encodage positionnel\n",
    "et le mécanisme d’attention multi-têtes. Nous utiliserons PyTorch pour\n",
    "cette implémentation.\n",
    "\n",
    "Rappel des Composants Clés\n",
    "--------------------------\n",
    "\n",
    "Un Transformer est composé de :\n",
    "\n",
    "-   **Encodage Positionnel** : Pour injecter des informations de\n",
    "    position dans les embeddings.\n",
    "-   **Encodeur** : Composé de plusieurs couches identiques, chacune avec\n",
    "    un mécanisme d’auto-attention multi-têtes et un réseau de neurones\n",
    "    feed-forward.\n",
    "-   **Décodeur** : Composé de plusieurs couches identiques, chacune avec\n",
    "    un mécanisme d’auto-attention masqué, un mécanisme d’attention\n",
    "    encodeur-décodeur et un réseau de neurones feed-forward.\n",
    "\n",
    "Implémentation en PyTorch\n",
    "-------------------------\n",
    "\n",
    "Nous allons définir les classes pour chaque composant, puis les\n",
    "assembler pour former le modèle Transformer complet.\n",
    "\n",
    "### 1. Encodage Positionnel\n",
    "\n",
    "Nous avons déjà vu cette classe dans le premier notebook, mais la voici\n",
    "à nouveau pour référence.\n",
    "\n",
    "``` python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "```\n",
    "\n",
    "### 2. Mécanisme d’Attention Multi-Têtes\n",
    "\n",
    "Nous avons également vu cette classe dans le deuxième notebook.\n",
    "\n",
    "``` python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nkhq\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nkhq,nvhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "```\n",
    "\n",
    "### 3. Couche d’Encodeur\n",
    "\n",
    "``` python\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.norm1(query + self.dropout(attention))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.norm2(x + self.dropout(forward))\n",
    "        return out\n",
    "```\n",
    "\n",
    "### 4. Encodeur\n",
    "\n",
    "``` python\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = PositionalEncoding(embed_size, max_length)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out\n",
    "```\n",
    "\n",
    "### 5. Couche de Décodeur\n",
    "\n",
    "``` python\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.norm(attention + self.dropout(x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out\n",
    "```\n",
    "\n",
    "### 6. Décodeur\n",
    "\n",
    "``` python\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = PositionalEncoding(embed_size, max_length)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        return out\n",
    "```\n",
    "\n",
    "### 7. Le Modèle Transformer Complet\n",
    "\n",
    "``` python\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, device=\"cuda\", max_length=100):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length)\n",
    "        self.decoder = Decoder(trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length)\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n",
    "    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "\n",
    "    src_pad_idx = 0\n",
    "    trg_pad_idx = 0\n",
    "    src_vocab_size = 10\n",
    "    trg_vocab_size = 10\n",
    "    model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n",
    "    out = model(x, trg[:, :-1])\n",
    "    print(f\"Output shape: {out.shape}\")\n",
    "```\n",
    "\n",
    "Conclusion\n",
    "----------\n",
    "\n",
    "Dans ce notebook, nous avons construit un modèle Transformer complet en\n",
    "assemblant les différents composants que nous avons étudiés\n",
    "précédemment. Cette implémentation de base vous donne une compréhension\n",
    "pratique de la façon dont ces modèles fonctionnent.\n",
    "\n",
    "Dans le prochain notebook, nous explorerons comment les Transformers\n",
    "pré-entraînés sont utilisés et comment les adapter à des tâches\n",
    "spécifiques grâce au fine-tuning."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
