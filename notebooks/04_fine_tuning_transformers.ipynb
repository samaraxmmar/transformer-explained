{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning des Transformers\n",
    "============================\n",
    "\n",
    "Bienvenue dans le quatrième et dernier notebook de la série\n",
    "“Transformers Explained” ! Dans ce notebook, nous allons explorer l’un\n",
    "des aspects les plus puissants des Transformers : le fine-tuning. Le\n",
    "fine-tuning permet d’adapter des modèles de Transformers pré-entraînés à\n",
    "des tâches spécifiques avec des ensembles de données plus petits, ce qui\n",
    "est extrêmement efficace et courant dans le domaine du TLN.\n",
    "\n",
    "Qu’est-ce que le Fine-tuning ?\n",
    "------------------------------\n",
    "\n",
    "Le fine-tuning est un processus qui consiste à prendre un modèle de\n",
    "réseau neuronal qui a déjà été entraîné sur un grand ensemble de données\n",
    "(souvent appelé pré-entraînement) et à l’entraîner davantage sur un\n",
    "ensemble de données plus petit et spécifique à une tâche. Pour les\n",
    "Transformers, cela signifie généralement prendre un modèle comme BERT,\n",
    "GPT, ou T5, qui a été pré-entraîné sur des téraoctets de texte, et\n",
    "l’adapter à des tâches comme la classification de texte, la\n",
    "reconnaissance d’entités nommées, la traduction, etc.\n",
    "\n",
    "Pourquoi le Fine-tuning est-il si efficace ?\n",
    "--------------------------------------------\n",
    "\n",
    "1.  **Transfert de connaissances** : Les modèles pré-entraînés ont déjà\n",
    "    appris des représentations riches et générales du langage à partir\n",
    "    de vastes corpus de texte. Ces connaissances sont transférées à la\n",
    "    nouvelle tâche.\n",
    "2.  **Moins de données nécessaires** : Puisque le modèle a déjà une\n",
    "    bonne compréhension du langage, il nécessite beaucoup moins de\n",
    "    données spécifiques à la tâche pour atteindre de bonnes\n",
    "    performances, par rapport à l’entraînement d’un modèle à partir de\n",
    "    zéro.\n",
    "3.  **Temps d’entraînement réduit** : Le fine-tuning est généralement\n",
    "    beaucoup plus rapide que l’entraînement à partir de zéro, car le\n",
    "    modèle a déjà convergé vers une bonne solution.\n",
    "\n",
    "Processus de Fine-tuning\n",
    "------------------------\n",
    "\n",
    "Le processus général de fine-tuning implique les étapes suivantes :\n",
    "\n",
    "1.  **Choisir un modèle pré-entraîné** : Sélectionner un modèle de\n",
    "    Transformer (par exemple, `bert-base-uncased`, `gpt2`, `t5-small`)\n",
    "    adapté à votre tâche et à vos ressources.\n",
    "2.  **Préparer les données** : Adapter votre ensemble de données\n",
    "    spécifique à la tâche au format attendu par le modèle (tokenisation,\n",
    "    ajout de tokens spéciaux, etc.).\n",
    "3.  **Modifier la couche de sortie** : Remplacer la couche de sortie du\n",
    "    modèle pré-entraîné par une nouvelle couche adaptée à votre tâche\n",
    "    (par exemple, une couche de classification pour la classification de\n",
    "    texte).\n",
    "4.  **Entraîner le modèle** : Entraîner le modèle sur votre ensemble de\n",
    "    données spécifique à la tâche, en ajustant les poids de toutes les\n",
    "    couches (ou seulement des dernières couches) avec un taux\n",
    "    d’apprentissage faible.\n",
    "\n",
    "Exemple de Fine-tuning avec Hugging Face Transformers\n",
    "-----------------------------------------------------\n",
    "\n",
    "La bibliothèque Hugging Face `transformers` est l’outil le plus\n",
    "populaire pour travailler avec les Transformers. Elle simplifie\n",
    "grandement le processus de fine-tuning.\n",
    "\n",
    "Nous allons montrer un exemple simplifié de fine-tuning pour une tâche\n",
    "de classification de texte en utilisant un modèle BERT.\n",
    "\n",
    "``` python\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# 1. Charger un modèle pré-entraîné et un tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2) # 2 classes: positif/négatif\n",
    "\n",
    "# 2. Préparer les données\n",
    "# Utilisons un petit dataset de classification de sentiments pour l'exemple\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_imdb = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_imdb[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_imdb[\"test\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "# Renommer la colonne 'label' en 'labels' pour être compatible avec Trainer\n",
    "small_train_dataset = small_train_dataset.rename_column(\"label\", \"labels\")\n",
    "small_eval_dataset = small_eval_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Supprimer les colonnes inutiles\n",
    "small_train_dataset = small_train_dataset.remove_columns([\"text\"])\n",
    "small_eval_dataset = small_eval_dataset.remove_columns([\"text\"])\n",
    "\n",
    "# Définir le format des données pour PyTorch\n",
    "small_train_dataset.set_format(\"torch\")\n",
    "small_eval_dataset.set_format(\"torch\")\n",
    "\n",
    "# 3. Définir les arguments d'entraînement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# 4. Définir une fonction de métrique (optionnel mais recommandé)\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 5. Créer l'objet Trainer et entraîner le modèle\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Début du fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning terminé.\")\n",
    "\n",
    "# Évaluer le modèle\n",
    "print(\"Évaluation du modèle...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Résultats de l'évaluation: {eval_results}\")\n",
    "\n",
    "# Exemple de prédiction\n",
    "text_to_classify = \"This movie was absolutely fantastic and I loved every minute of it!\"\n",
    "inputs = tokenizer(text_to_classify, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "predicted_label = \"positif\" if predicted_class_id == 1 else \"négatif\"\n",
    "\n",
    "print(f\"Texte: \\\"{text_to_classify}\\\"\")\n",
    "print(f\"Sentiment prédit: {predicted_label}\")\n",
    "\n",
    "text_to_classify_neg = \"This was a terrible film, I hated it.\"\n",
    "inputs_neg = tokenizer(text_to_classify_neg, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_neg = model(**inputs_neg).logits\n",
    "\n",
    "predicted_class_id_neg = logits_neg.argmax().item()\n",
    "predicted_label_neg = \"positif\" if predicted_class_id_neg == 1 else \"négatif\"\n",
    "\n",
    "print(f\"Texte: \\\"{text_to_classify_neg}\\\"\")\n",
    "print(f\"Sentiment prédit: {predicted_label_neg}\")\n",
    "```\n",
    "\n",
    "Conclusion\n",
    "----------\n",
    "\n",
    "Le fine-tuning est une technique essentielle pour tirer parti de la\n",
    "puissance des modèles de Transformers pré-entraînés. Il permet d’obtenir\n",
    "des performances de pointe sur une grande variété de tâches de TLN avec\n",
    "des efforts et des ressources d’entraînement considérablement réduits.\n",
    "\n",
    "Ce dépôt vous a fourni une base solide pour comprendre les Transformers,\n",
    "de leurs principes fondamentaux à leur application pratique via le\n",
    "fine-tuning. Nous espérons que cela vous aidera dans votre parcours\n",
    "d’apprentissage du TLN !"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
